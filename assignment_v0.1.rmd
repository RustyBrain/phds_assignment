---
title: "Composite Health Index for England"
author: "Russell Plunkett"
date: "8 April 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require("pacman")) install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(arulesViz, betareg, car, caret, dplyr,
               fingertipsR, geojsonio, ggcorrplot, ggplot2, htmltools,
               leaflet, mi, psycho, readr, rmapshaper, tibble ,tidyr)
# library(arulesViz)
# library(betareg)
# library(car)
# library(caret)
# library(dplyr)
# library(fingertipsR)
# library(geojsonio)
# library(ggcorrplot)
# library(ggplot2)
# library(htmltools)
# library(leaflet)
# library(mi)
# library(psycho)
# library(readr)
# library(rmapshaper)
# library(tibble)
# library(tidyr)

# listOfPackages <- c("arulesViz","betareg","car","caret","dplyr",
#                     "fingertipsR","geojsonio","ggcorrplot","ggplot2", "htmltools",
#                     "leaflet","mi", "psycho" ,"readr", "tibble" ,"tidyr")
# for (i in listOfPackages){
#      if(! i %in% installed.packages()){
#          install.packages(i, dependencies = TRUE)
#     }
#     print(i)  
#     library(i)
# }
```

## Get and Manipulate the data to retreive mean Z score:

```{r imd_data}
# Get the indicies of multiple deprivation at LSOA level.
imd_df <- read_csv('https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/467774/File_7_ID_2015_All_ranks__deciles_and_scores_for_the_Indices_of_Deprivation__and_population_denominators.csv')

# Add UTLA mapping 
lsoa_utla_lookup <- read_csv('https://opendata.arcgis.com/datasets/95ecb220a30e41d5ae759cd1c9aa929f_0.csv')
lsoa_utla_lookup <- lsoa_utla_lookup %>%
  select(LSOA11CD, UTLA18CD)
imd_df <- merge(x = imd_df, y = lsoa_utla_lookup, by.x = 'LSOA code (2011)', by.y = 'LSOA11CD')

# Select only the subdomain totals
pop_weighted_ave_domains <- imd_df %>%
  select("Income Score (rate)", "Employment Score (rate)", "Education, Skills and Training Score", "Health Deprivation and Disability Score",
                 "Crime Score", "Barriers to Housing and Services Score", "Living Environment Score", 
                 "Income Deprivation Affecting Children Index (IDACI) Score (rate)", 
                 "Income Deprivation Affecting Older People (IDAOPI) Score (rate)", "Children and Young People Sub-domain Score",
                 "Adult Skills Sub-domain Score", "Geographical Barriers Sub-domain Score", "Wider Barriers Sub-domain Score", 
                 "Indoors Sub-domain Score", "Outdoors Sub-domain Score", "Total population: mid 2012 (excluding prisoners)")

# Create population weighted averages for each UTLA
pop_weighted_ave_domains <- pop_weighted_ave_domains[, 1:15] / pop_weighted_ave_domains[,16]
pop_weighted_ave_domains$UTLA18CD <- imd_df$UTLA18CD
pop_weighted_ave_domains <- pop_weighted_ave_domains %>%
  group_by(UTLA18CD) %>%
  summarise_each(funs(mean))

# Create z scores of deprivation matrix and reverse polarity as low is good on the deprivation scores
z_data_imd <- pop_weighted_ave_domains %>%
  standardize() %>%
  mutate_each(funs(-.), -one_of("UTLA18CD"))
```

```{r fingertips_data}
# Get data
# Seleceted indicators from a variety of fingertips profiles selected to fulfill the Marmot definitions of Social Determinants, Health Outcomes, and Risk Factors

early_years_indicators <- indicators(DomainID = 1938133223)
social_determinants_df <- fingertips_data(IndicatorID = c(as.vector(early_years_indicators$IndicatorID), 93376, 22304, 10501, 90282, 91126, 91133, 92899, 90638, 93103, 93131, 93175, 91463, 90244, 90245, 92447, 91116, 92785, 91524, 91307), rank = T)

prevalance_estimates = indicators(ProfileID = 37)
mortality_indicators = indicators(DomainID=c(1000044, 1000049))
extra_indicators = as.vector(rbind(as.vector(prevalance_estimates$IndicatorID), as.vector(mortality_indicators$IndicatorID)))
health_outcomes_df = fingertips_data(IndicatorID = c(extra_indicators, 20101, 92718, 92724, 92725, 90287, 90641), rank = T)

behav_risk_factors = indicators(DomainID=c(1938132694, 1938133001))
inds <- c(as.vector(behav_risk_factors$IndicatorID), 30101, 113, 1203, 93085, 90323, 93077, 92527, 92937, 92588, 92443)
risk_factors_df = fingertips_data(IndicatorID = inds, rank = T)

```

```{r manipulate_data}

analyze_df <- function(df, dataset){
  # Create a data frame of indicator polarities
  polarities <- df %>%
    select(IndicatorID, Polarity) %>%
    distinct() %>%
    spread(key = IndicatorID, value = Polarity)
    
  
  # Select as most recent for each indicator
  most_recent_data <- df %>%
    group_by(IndicatorID) %>%
    filter(TimeperiodSortable == max(TimeperiodSortable)) %>%
    ungroup()
  
  # Select County & UA
  most_recent_data <- most_recent_data[grep('County', most_recent_data$AreaType), ]
  
  
  # Transpose to wide data
  wide_df <- most_recent_data %>%
    select(AreaCode, IndicatorID, Value) %>%
    tibble::rowid_to_column() %>%
    group_by(IndicatorID, AreaCode) %>%
    spread(key = IndicatorID, value = Value) %>%
    summarise_all(funs(na.omit(.)[1]))
  rownames(wide_df) <- wide_df$AreaCode
  wide_df <- wide_df %>%
    select(-rowid) # Remove row id column
  
  # Remove areas with over 50% missing data - areas with this are unlikely to add any information as their values are more imputed than not
  wide_df$proportna <- apply(wide_df, 1, function(x) sum(is.na(x)) / dim(wide_df)[2])
  wide_df <- wide_df[wide_df$proportna < 0.5,] 
  wide_df <- wide_df %>% select(-proportna)
  
  
  # Remove indicators with over 80% missing data - rather arbatrary number at the moment
  wide_df <- wide_df[colSums(is.na(wide_df)) / dim(wide_df)[1] < 0.2]
  
  
  # impute missing data - using a Bayesian framework, with 30 inerations and 4 chains. A random seed was chosen to ensure reproducability. The bootstrap was envoked as a method of selecting random imputations. 
  # http://www.stat.columbia.edu/~gelman/research/published/mipaper.pdf
  to_impute <- wide_df
  rownames(to_impute) <- to_impute$AreaCode
  to_impute <- to_impute %>% select(-AreaCode) 
  imputed <- mi(as.data.frame(to_impute), seed = 225)
  summary(imputed)
  image(imputed) # Show heatmap of imputed values in heatmap
  imputed_df <- mi::complete(imputed, m = 1) # Retrieve the dataframe
  imputed_df <- select(imputed_df, -contains("missing")) # Remove the boolean 'missing' columns. 
  
  
  # Test normality - using shapiro-wilks with an alpha of 0.001. Alpha chosen as Central Limit Theorum would suggest that with n=150, data should tend towards being normally distributed. 
  # These tests are to catch extremes where normality is violated and to transform them to normalise using square root, log, and box-cox transformations in turn. The box-cox transformation
  # used was the Yeo-Johnson variation (yjPower()) as some of the data contains 0 values.
  # Yeo, In-Kwon and Johnson, Richard (2000) A new family of power transformations to improve normality or symmetry. Biometrika, 87, 954-959.
  # test_normaility_and_transform <- function(column_data){
  # 
  #   norm_test <- shapiro.test(column_data)
  #   print(norm_test)
  #   if (norm_test$p.value < 0.001) {
  #     norm_test_sr <- shapiro.test(sqrt(column_data))
  #     if (norm_test_sr$p.value < 0.001) {
  #       column_data[column_data == 0] <- 0.00001
  #       if (is.numeric(column_data == FALSE)){
  #         print(column_data)
  #       }
  #       norm_test_lg <- shapiro.test(log(column_data))
  #       if (norm_test_lg$p.value < 0.001) {
  #         
  #         norm_test_bc <- shapiro.test(yjPower(column_data, lambda = 1))
  #         
  #         print(norm_test_bc)
  #         if (norm_test_bc$p.value < 0.001) {
  #           return(NA)
  #         } 
  #         else { return(yjPower(column_data, lambda = 1))} }
  #       else {
  #         return(log(column_data))
  #       }
  #     } 
  #     else {
  #       return(sqrt(column_data))
  #     }
  #   }
  #   else {
  #     return(column_data)
  #   }
  # }
  # 
  # 
  # # normality <- imputed_df %>% summarise_all(.funs = funs(statistic = shapiro.test(.)$statistic, p.value = shapiro.test(.)$p.value))
  # # Normalise data.
  # normalised <- data.frame(imputed_df, lapply(imputed_df, test_normaility_and_transform))
  # print('normalised finished')
  # 
  # #Some transformed data contains NA values, and therefore these values were imputed using the same method as above. 
  # 
  # norm_reduced <- select(normalised, contains(".1"))
  # norm_reduced <- norm_reduced[!sapply(norm_reduced, function(x) all(is.na(x)))]
  # norm_reduced <- mi(norm_reduced, seed = 225)
  # print('is this here?')
  # norm_reduced <- mi::complete(norm_reduced, m = 1)
  # norm_reduced <- select(norm_reduced, -contains("missing"))
  # print('norm reducued here')
  # 
  # 
  # print(norm_reduced)
  norm_reduced <- imputed_df
  print(norm_reduced)
  # Test colinearity - needs a lot of work!
  corr <- cor(norm_reduced)
  ggcorrplot(corr, hc.order = TRUE, type = "lower",
             outline.col = "white",
             ggtheme = ggplot2::theme_gray,
             colors = c("#6D9EC1", "white", "#E46726"),
             insig = "blank")
  hc <- findCorrelation(corr)
  hc <- sort(hc)
  if (length(hc) == 0){
    reduced_data <- norm_reduced
  } else {
    reduced_data <- norm_reduced[, -c(hc)]
    corr_reduced <- cor(reduced_data)
    ggcorrplot(corr_reduced, hc.order = TRUE, type = "lower",
               outline.col = "white",
               ggtheme = ggplot2::theme_gray,
               colors = c("#6D9EC1", "white", "#E46726"),
               insig = "blank")
  }



  # Polarity check and inversion. If 'Low is good', then the scores are inverted to ensure the same direction of performance in the data. 
  polarity_check <- function(col_name){
    if (grepl(polarities[[col_name]], "RAG - Low is good")) {
      z_data[[col_name]] <- -z_data[[col_name]]
    } else {
      z_data[[col_name]] <- z_data[[col_name]]
    }
  }



  # Create Z scores using psycho package's standardize() function. 
  # Makowski, (2018). The psycho Package: an Efficient and Publishing-Oriented Workflow for Psychological Science. Journal of Open Source Software, 3(22), 470. https://doi.org/10.21105/joss.00470
  z_data <- reduced_data %>% standardize()
  names(z_data) <- substring(names(z_data), 2)
  z_data <- z_data %>% 
                rename_at(.vars = vars(ends_with(".1")),
                .funs = funs(sub("[.]1$", "", .)))
  polarities <- polarities[, colnames(z_data)]
  
  
  # Create a dataframe of z-scores, create mean score & add area codes on. 
  z_data <- data.frame(z_data, lapply(colnames(z_data), polarity_check))
  z_data <- z_data[, -c(1:ncol(polarities))]
  colnames(z_data) <- colnames(polarities)
  
  # Get metadata for indicators and set them to global variables to use outside the function
  if (dataset == 'r'){
    risk_factors_metadata <<- indicator_metadata(IndicatorID = colnames(z_data))
  } else if (dataset == 's') {
    soc_det_metadata <<- indicator_metadata(IndicatorID = colnames(z_data))
  } else if (dataset == 'h') {
    health_outcomes_metadata <<- indicator_metadata(IndicatorID = colnames(z_data))
  }
  z_data$mean <- apply(z_data, 1, mean)
  z_data$AreaCode <- wide_df$AreaCode
  
  
  # z_mean <- z_data %>% select(mean, AreaCode)
  return(z_data)
}

health_outcomes_processed <- analyze_df(health_outcomes_df, 'h')
risk_factors_processed <- analyze_df(risk_factors_df, 'r')

# Add imd to social determinants and recalc mean
soc_det_processed <- analyze_df(social_determinants_df, 's')
soc_det_processed <- merge(soc_det_processed, z_data_imd, by.x = 'AreaCode', by.y = 'UTLA18CD')
soc_det_processed$mean <- apply(soc_det_processed[, !(names(soc_det_processed) %in% c('mean', 'AreaCode'))], 1, mean)

z_means <- data.frame(health_outcomes_processed$mean, soc_det_processed$mean, risk_factors_processed$mean, risk_factors_processed$AreaCode)
names(z_means) <- c("health_outcomes", "social_determinants", "risk_factors", "AreaCode")
z_means$grand_mean <- apply(z_means[, !(names(z_means) %in% c('AreaCode'))], 1, mean)
z_means$health_outcomes.rank <- rank(z_means$health_outcomes, na.last = FALSE)
z_means$social_determinants.rank <- rank(z_means$social_determinants, na.last = FALSE)
z_means$risk_factors.rank <- rank(z_means$risk_factors, na.last = FALSE)
z_means$overall.rank <- rank(z_means$grand_mean, na.last = FALSE)
```

## Plot of interactive map


```{r get_shape_data, out.width = '100%'}
utlas <- geojson_read("https://opendata.arcgis.com/datasets/d3d7b7538c934cf29db791a705631e24_0.geojson", what = 'sp')
# drop Wales
utlas <- utlas[grep('E', utlas$ctyua17cd), ]
utlas <- ms_simplify(utlas)
utlas@data <- left_join(utlas@data, z_means, by = c("ctyua17cd" = "AreaCode"))
pal_grand <- colorNumeric(palette = "Spectral", domain = utlas$grand_mean)
pal_soc <- colorNumeric(palette = "Spectral", domain = utlas$social_determinants)
pal_risk <- colorNumeric(palette = "Spectral", domain = utlas$risk_factors)
pal_outcomes <- colorNumeric(palette = "Spectral", domain = utlas$health_outcomes)
labs_grand_mean <- lapply(seq(nrow(utlas@data)), function(i) {
  paste0( '<p>', utlas@data[i, "ctyua17nm"], '<p></p>', 
          'Health Index Score:', round(utlas@data[i, 'grand_mean'], 3),'</p><p>', 
          'Rank:', utlas@data[i, 'overall.rank'], '</p>' ) 
})
labs_soc <- lapply(seq(nrow(utlas@data)), function(i) {
  paste0( '<p>', utlas@data[i, "ctyua17nm"], '<p></p>', 
          'Social Determinants Score:', round(utlas@data[i, 'social_determinants'], 3),'</p><p>', 
          'Rank:', utlas@data[i, 'social_determinants.rank'], '</p>' ) 
})
labs_risk <- lapply(seq(nrow(utlas@data)), function(i) {
  paste0( '<p>', utlas@data[i, "ctyua17nm"], '<p></p>', 
          'Risk Factors Score:', round(utlas@data[i, 'risk_factors'], 3),'</p><p>', 
          'Rank:', utlas@data[i, 'risk_factors.rank'], '</p>' ) 
})
labs_outcomes <- lapply(seq(nrow(utlas@data)), function(i) {
  paste0( '<p>', utlas@data[i, "ctyua17nm"], '<p></p>', 
          'Health Outcomes Score:', round(utlas@data[i, 'health_outcomes'], 3),'</p><p>', 
          'Rank:', utlas@data[i, 'health_outcomes.rank'], '</p>' ) 
})
map1 <- leaflet(utlas) %>%
  # Base groups
  addTiles(group = "OSM (default)") %>%
  # Overlay groups
  # addCircles(~long, ~lat, ~10^mag/5, stroke = F, group = "Quakes") %>%
  addPolygons(
    weight = 2, fillOpacity = 0.8, color = ~pal_grand(grand_mean), group = "Health Index - Total", label = lapply(labs_grand_mean, HTML)) %>%
  
  addPolygons(
    weight = 2, fillOpacity = 0.8, color = ~pal_soc(social_determinants), group = "Social Determinants", label = lapply(labs_soc, HTML)) %>%
  addPolygons(
    weight = 2, fillOpacity = 0.8, color = ~pal_risk(risk_factors), group = "Risk Factors", label = lapply(labs_risk, HTML)) %>%
  addPolygons(
    weight = 2, fillOpacity = 0.8, color = ~pal_outcomes(health_outcomes), group = "Health Outcomes", label = lapply(labs_outcomes, HTML)) %>%


  # Layers control
  addLayersControl(
    baseGroups = c("Health Index - Total", "Social Determinants", "Risk Factors", "Health Outcomes"),
    overlayGroups = c("OSM (default)"),
    options = layersControlOptions(collapsed = FALSE)) %>%
  addLegend("bottomright", pal = pal_grand, values = utlas$grand_mean, title = "Mean Z Scores for Index", opacity = 1)
map1
```


